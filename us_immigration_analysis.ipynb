{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering Capstone Project\n",
    "\n",
    "## US I94 Immigration Data Lake\n",
    "\n",
    "### Project Summary\n",
    "This project performs ETL operations on Udacity provided I94 immigration and demographics datasets using Pyspark. It generates a Star Schema in parquet file at the end following Data Lake's schema-on-read semantics.\n",
    "\n",
    "This notebooks performs Exploratory Data Analysis on used datasets.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check readme for installation and env setup\n",
    "import configparser\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from etl import (\n",
    "    get_spark_session,\n",
    "    get_immigration_data,\n",
    "    get_us_demographics_data,\n",
    "    get_i94_countries,\n",
    "    get_i94_ports,\n",
    "    get_i94_states,\n",
    "    get_i94_travel_modes,\n",
    "    get_i94_visas,\n",
    "    clean_immigration_data,\n",
    "    clean_us_demographics_data,\n",
    "    clean_ports_data,\n",
    "    clean_countries_data,\n",
    "    clean_states_data,\n",
    "    create_immigration_fact_table,\n",
    "    create_city_demographics_dim_table,\n",
    "    logger\n",
    ")\n",
    "\n",
    "logger.setLevel(logging.WARN) # To suppress etl logs in notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "*Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc.*\n",
    "\n",
    "I plan to create a data lake using Pyspark about immigrants destinations in US. To achieve this, I've used I94 immigrations dataset along with US demographics and ISO-3166 country codes datasets. Processed data lake could be used to analyse immigration trends at particular time periods and origin of the travelers. Output is generated in `Apache Parquet` columnar format for better performance on aggregation queries.\n",
    "\n",
    "**Tools/Tech Used**: Python, Apache Spark (PySpark), Pandas\n",
    "\n",
    "#### Describe and Gather Data \n",
    "*Describe the data sets you're using. Where did it come from? What type of information is included?*\n",
    "\n",
    "Following datasets are used for this project:\n",
    "- **I94 Immigration Data 2016:** This data comes from the US National Tourism and Trade Office.\n",
    "    - Source: https://travel.trade.gov/research/reports/i94/historical/2016.html\n",
    "    - Note this data is behind a paywall and provided by Udacity for this project.\n",
    "    - Dataset consists of 12 files containing data for each month. Each file has around 3 million rows and 28 columns. A data dictionary explaining columns is also included at `data/I94_SAS_Labels_Descriptions.SAS`.\n",
    "    - Sample CSV: `data/input/immigration_data_sample.csv`\n",
    "    - NOTE: I've used sample sas dataset provided in `sas_data` dir in workspace by Udacity. This data contains ~3MM rows which satisfies the requirement of at least 1MM rows. It contains data for April 2016 only.\n",
    "- **U.S. City Demographic Data:** This data comes from OpenSoft.\n",
    "    - Source: https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### 1.1 Read config and load sample from datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat\n"
     ]
    }
   ],
   "source": [
    "# Because the immigration data has 28 columns\n",
    "pd.set_option('display.max_columns', 28)\n",
    "\n",
    "\n",
    "# Read config\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('capstone.cfg'))\n",
    "\n",
    "I94_DATA_FILE_PATH = config['DATA']['I94_DATA_FILE_PATH']\n",
    "print(I94_DATA_FILE_PATH)\n",
    "# df = pd.read_sas(I94_DATA_FILE_PATH, format='sas7bdat', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I94_LOCAL_DATA_DIR:  data/input/sas_data/\n"
     ]
    }
   ],
   "source": [
    "# Read local parquet dataset in `df`\n",
    "I94_LOCAL_DATA_DIR = config['DATA']['I94_LOCAL_DATA_DIR']\n",
    "print(\"I94_LOCAL_DATA_DIR: \", I94_LOCAL_DATA_DIR)\n",
    "\n",
    "# To reduce memory usage locally\n",
    "# df = pd.read_parquet(I94_LOCAL_DATA_DIR).sample(n=1000)\n",
    "# df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# df.sample(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### 1.2 Configure Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created\n"
     ]
    }
   ],
   "source": [
    "# spark = SparkSession.builder \\\n",
    "#             .appName(\"Capstone Project - Immigration Dataset\") \\\n",
    "#             .config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\") \\\n",
    "#             .enableHiveSupport() \\\n",
    "#             .getOrCreate()\n",
    "spark = get_spark_session()\n",
    "print(\"Spark session created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### 1.3 Show sample immigration data\n",
    "\n",
    "Read April 2016 file in spark dataframe (same as Pandas df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading apr sas file in Spark df\n",
    "\n",
    "# immigration_df = spark.read.format('com.github.saurfang.sas.spark').load(I94_DATA_FILE_PATH)\n",
    "\n",
    "immigration_df = get_immigration_data(spark)\n",
    "immigration_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paritions:  4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5748517.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20582.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160430</td>\n",
       "      <td>SYD</td>\n",
       "      <td>None</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>QF</td>\n",
       "      <td>9.495387e+10</td>\n",
       "      <td>00011</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5748518.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NV</td>\n",
       "      <td>20591.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160430</td>\n",
       "      <td>SYD</td>\n",
       "      <td>None</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1984.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>VA</td>\n",
       "      <td>9.495562e+10</td>\n",
       "      <td>00007</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5748519.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>WA</td>\n",
       "      <td>20582.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160430</td>\n",
       "      <td>SYD</td>\n",
       "      <td>None</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1987.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>DL</td>\n",
       "      <td>9.495641e+10</td>\n",
       "      <td>00040</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5748520.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>WA</td>\n",
       "      <td>20588.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160430</td>\n",
       "      <td>SYD</td>\n",
       "      <td>None</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1987.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>DL</td>\n",
       "      <td>9.495645e+10</td>\n",
       "      <td>00040</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5748521.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>WA</td>\n",
       "      <td>20588.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160430</td>\n",
       "      <td>SYD</td>\n",
       "      <td>None</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>DL</td>\n",
       "      <td>9.495639e+10</td>\n",
       "      <td>00040</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5748522.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>464.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>20579.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160430</td>\n",
       "      <td>ACK</td>\n",
       "      <td>None</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1959.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>NZ</td>\n",
       "      <td>9.498180e+10</td>\n",
       "      <td>00010</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5748523.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>464.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>20586.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160430</td>\n",
       "      <td>ACK</td>\n",
       "      <td>None</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1950.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>NZ</td>\n",
       "      <td>9.497969e+10</td>\n",
       "      <td>00010</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5748524.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>464.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>20586.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160430</td>\n",
       "      <td>ACK</td>\n",
       "      <td>None</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1975.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>NZ</td>\n",
       "      <td>9.497975e+10</td>\n",
       "      <td>00010</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5748525.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>464.0</td>\n",
       "      <td>HOU</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>20581.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160430</td>\n",
       "      <td>ACK</td>\n",
       "      <td>None</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1989.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>NZ</td>\n",
       "      <td>9.497325e+10</td>\n",
       "      <td>00028</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5748526.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>464.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20581.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160430</td>\n",
       "      <td>ACK</td>\n",
       "      <td>None</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>NZ</td>\n",
       "      <td>9.501355e+10</td>\n",
       "      <td>00002</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0       1    2      3      4    5        6    7   8        9    10  \\\n",
       "0  5748517.0  2016.0  4.0  245.0  438.0  LOS  20574.0  1.0  CA  20582.0  40.0   \n",
       "1  5748518.0  2016.0  4.0  245.0  438.0  LOS  20574.0  1.0  NV  20591.0  32.0   \n",
       "2  5748519.0  2016.0  4.0  245.0  438.0  LOS  20574.0  1.0  WA  20582.0  29.0   \n",
       "3  5748520.0  2016.0  4.0  245.0  438.0  LOS  20574.0  1.0  WA  20588.0  29.0   \n",
       "4  5748521.0  2016.0  4.0  245.0  438.0  LOS  20574.0  1.0  WA  20588.0  28.0   \n",
       "5  5748522.0  2016.0  4.0  245.0  464.0  HHW  20574.0  1.0  HI  20579.0  57.0   \n",
       "6  5748523.0  2016.0  4.0  245.0  464.0  HHW  20574.0  1.0  HI  20586.0  66.0   \n",
       "7  5748524.0  2016.0  4.0  245.0  464.0  HHW  20574.0  1.0  HI  20586.0  41.0   \n",
       "8  5748525.0  2016.0  4.0  245.0  464.0  HOU  20574.0  1.0  FL  20581.0  27.0   \n",
       "9  5748526.0  2016.0  4.0  245.0  464.0  LOS  20574.0  1.0  CA  20581.0  26.0   \n",
       "\n",
       "    11   12        13   14    15 16 17    18 19      20        21 22    23  \\\n",
       "0  1.0  1.0  20160430  SYD  None  G  O  None  M  1976.0  10292016  F  None   \n",
       "1  1.0  1.0  20160430  SYD  None  G  O  None  M  1984.0  10292016  F  None   \n",
       "2  1.0  1.0  20160430  SYD  None  G  O  None  M  1987.0  10292016  M  None   \n",
       "3  1.0  1.0  20160430  SYD  None  G  O  None  M  1987.0  10292016  F  None   \n",
       "4  1.0  1.0  20160430  SYD  None  G  O  None  M  1988.0  10292016  M  None   \n",
       "5  2.0  1.0  20160430  ACK  None  G  O  None  M  1959.0  10292016  M  None   \n",
       "6  2.0  1.0  20160430  ACK  None  G  O  None  M  1950.0  10292016  F  None   \n",
       "7  2.0  1.0  20160430  ACK  None  G  O  None  M  1975.0  10292016  F  None   \n",
       "8  2.0  1.0  20160430  ACK  None  G  O  None  M  1989.0  10292016  M  None   \n",
       "9  2.0  1.0  20160430  ACK  None  G  O  None  M  1990.0  10292016  F  None   \n",
       "\n",
       "   24            25     26  27  \n",
       "0  QF  9.495387e+10  00011  B1  \n",
       "1  VA  9.495562e+10  00007  B1  \n",
       "2  DL  9.495641e+10  00040  B1  \n",
       "3  DL  9.495645e+10  00040  B1  \n",
       "4  DL  9.495639e+10  00040  B1  \n",
       "5  NZ  9.498180e+10  00010  B2  \n",
       "6  NZ  9.497969e+10  00010  B2  \n",
       "7  NZ  9.497975e+10  00010  B2  \n",
       "8  NZ  9.497325e+10  00028  B2  \n",
       "9  NZ  9.501355e+10  00002  B2  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Paritions: \", immigration_df.rdd.getNumPartitions())\n",
    "df_pd = pd.DataFrame(immigration_df.head(10))\n",
    "df_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### 2.1 Explore the Data\n",
    "*Identify data quality issues, like missing values, duplicate data, etc.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data sets\n",
    "us_demographics_df = get_us_demographics_data(spark)\n",
    "countries_df = get_i94_countries(spark)\n",
    "ports_df = get_i94_ports(spark)\n",
    "states_df = get_i94_states(spark)\n",
    "travel_modes_df = get_i94_travel_modes(spark)\n",
    "visa_categories_df = get_i94_visas(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "567"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_demographics_df.select(\"city\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Cleaning Steps\n",
    "Document steps necessary to clean the data\n",
    "\n",
    "Explained in readme/etl.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing cleaning tasks here\n",
    "cleaned_immigration_df = clean_immigration_data(immigration_df)\n",
    "cleaned_us_demographics_df = clean_us_demographics_data(us_demographics_df)\n",
    "cleaned_ports_df = clean_ports_data(ports_df)\n",
    "cleaned_countries_df = clean_countries_data(countries_df)\n",
    "cleaned_states_df = clean_states_data(states_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_us_demographics_df.createOrReplaceTempView(\"cleaned_us_demographics_df\")\n",
    "us_demographics_df.createOrReplaceTempView(\"us_demographics_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- median_age: double (nullable = true)\n",
      " |-- male_population: integer (nullable = true)\n",
      " |-- female_population: integer (nullable = true)\n",
      " |-- total_population: integer (nullable = true)\n",
      " |-- number_of_veterans: integer (nullable = true)\n",
      " |-- foreign_born: integer (nullable = true)\n",
      " |-- average_household_size: double (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_us_demographics_df.printSchema()\n",
    "cleaned_us_demographics_df.select(\"state_code\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- median_age: double (nullable = true)\n",
      " |-- male_population: integer (nullable = true)\n",
      " |-- female_population: integer (nullable = true)\n",
      " |-- total_population: integer (nullable = true)\n",
      " |-- number_of_veterans: integer (nullable = true)\n",
      " |-- foreign_born: integer (nullable = true)\n",
      " |-- average_household_size: double (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_demographics_df.printSchema()\n",
    "us_demographics_df.select(\"state_code\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- port_code: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_ports_df.printSchema()\n",
    "cleaned_ports_df.select(\"state_code\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|count(DISTINCT city)|\n",
      "+--------------------+\n",
      "|                 559|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT COUNT(DISTINCT city)\n",
    "    FROM cleaned_us_demographics_df\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|        city|      state|\n",
      "+------------+-----------+\n",
      "|    San Juan|Puerto Rico|\n",
      "|The Villages|    Florida|\n",
      "|    Mayagüez|Puerto Rico|\n",
      "|    Guaynabo|Puerto Rico|\n",
      "|       Ponce|Puerto Rico|\n",
      "|      Caguas|Puerto Rico|\n",
      "|     Bayamón|Puerto Rico|\n",
      "|    Carolina|Puerto Rico|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT DISTINCT demo.city, demo.state\n",
    "    FROM us_demographics_df demo\n",
    "        LEFT OUTER JOIN cleaned_us_demographics_df cdf \n",
    "            ON lower(demo.city) = lower(cdf.city)\n",
    "    WHERE cdf.city IS NULL\n",
    "        \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     588|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark.sql(\"\"\"\n",
    "#     SELECT\n",
    "#         COUNT(sud.city)\n",
    "#     FROM cleaned_us_demographics_df sud\n",
    "#     GROUP BY sud.city, sud.state_code\n",
    "# \"\"\").count()\n",
    "\n",
    "aggregated_df = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        sud.city,\n",
    "        sud.state_code,\n",
    "        SUM(sud.male_population) AS male_population,\n",
    "        SUM(sud.female_population) AS female_population,\n",
    "        SUM(sud.total_population) AS total_population,\n",
    "        SUM(sud.number_of_veterans) AS number_of_veterans,\n",
    "        SUM(sud.foreign_born) AS num_foreign_born\n",
    "    FROM staging_us_demographics sud\n",
    "    GROUP BY sud.city, sud.state_code\n",
    "\"\"\")\n",
    "aggregated_df.createOrReplaceTempView('combined_demographics')\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        COUNT(*)\n",
    "    FROM combined_demographics\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_ports_df.createOrReplaceTempView(\"cleaned_ports_df\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        sp.port_code AS port_code,\n",
    "        cd.*\n",
    "    FROM cleaned_ports_df sp\n",
    "        JOIN combined_demographics cd \n",
    "            ON lower(cd.city) = lower(sp.city) AND cd.state_code = sp.state_code\n",
    "\"\"\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "*Map out the conceptual data model and explain why you chose that model*\n",
    "\n",
    "Added in readme.\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "*List the steps necessary to pipeline the data into the chosen data model*\n",
    "\n",
    "Added in readme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating immigrations fact\n",
    "immigration_fact_table = create_immigration_fact_table(spark, cleaned_immigration_df, cleaned_countries_df,\n",
    "                                                           cleaned_states_df, cleaned_ports_df, visa_categories_df,\n",
    "                                                           travel_modes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create port demographics dimension table\n",
    "city_demographics_table = create_city_demographics_dim_table(spark, cleaned_us_demographics_df, cleaned_ports_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 50) # To reduce data shuffling locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tables\n",
    "immigration_fact_table.createOrReplaceTempView(\"fact_immigrations\")\n",
    "city_demographics_table.createOrReplaceTempView(\"dim_city_demographics\")\n",
    "ports_df.createOrReplaceTempView(\"dim_ports\")\n",
    "countries_df.createOrReplaceTempView(\"dim_country\")\n",
    "states_df.createOrReplaceTempView(\"dim_us_state\")\n",
    "visa_categories_df.createOrReplaceTempView(\"dim_visa_category\")\n",
    "travel_modes_df.createOrReplaceTempView(\"dim_travel_mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 2823272|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as count\n",
    "    FROM fact_immigrations\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(cicid=3386206.0, entry_year=2016.0, entry_month=4.0, origin_country_code='438', port_code='FMY', arrival_date='2016-04-18', travel_mode_code='1', us_state_code='AZ', departure_date='2016-04-24', age=54.0, visa_category_code='1', occupation=None, gender='M', birth_year='1962.0', entry_date='07162016', airline='QF', admission_number=56359932933.0, flight_number='00093', visa_type='WB')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: its quite an expensive operation locally\n",
    "immigration_fact_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     660|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT count(*) FROM dim_ports\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     113|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT count(*) FROM dim_city_demographics\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     289|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT count(*) FROM dim_country\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|      55|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT count(*) FROM dim_us_state\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       3|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT count(*) FROM dim_visa_category\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       4|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT count(*) FROM dim_travel_mode\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|visa_category_code|\n",
      "+------------------+\n",
      "|                 3|\n",
      "|                 2|\n",
      "|                 1|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_fact_table.select(\"visa_category_code\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- port_code: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- male_population: long (nullable = true)\n",
      " |-- female_population: long (nullable = true)\n",
      " |-- total_population: long (nullable = true)\n",
      " |-- number_of_veterans: long (nullable = true)\n",
      " |-- num_foreign_born: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "city_demographics_table.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people.\n",
    " \n",
    " Check readme.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The data model is appropriate for the identified purpose.\n",
    "Queries:\n",
    "- Which city was most visited in a specific month?\n",
    "- From which country (or countries) travelers originate? Top countries of origin.\n",
    "- Top countries from where students are coming?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which city was most visited in a specific month?\n",
    "# This data is from April file, let's try for it\n",
    "# numeric code for April is 4\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        tvc.port_code,\n",
    "        tvc.immigrant_visits,\n",
    "        dcd.city,\n",
    "        dcd.state_code,\n",
    "        dcd.total_population\n",
    "    FROM\n",
    "        (SELECT \n",
    "            fi.port_code AS port_code, \n",
    "            COUNT(*) AS immigrant_visits\n",
    "        FROM fact_immigrations fi \n",
    "        WHERE fi.entry_month = 4\n",
    "        GROUP BY fi.port_code\n",
    "        ORDER BY immigrant_visits DESC\n",
    "        LIMIT 10\n",
    "        ) AS tvc \n",
    "    JOIN dim_city_demographics dcd\n",
    "        ON dcd.port_code = tvc.port_code\n",
    "    ORDER BY tvc.immigrant_visits DESC\n",
    "\"\"\").show()\n",
    "# Showing top 10 visited cities here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------+------------+--------------------+\n",
      "|origin_country_code|country_visitors|country_code|        country_name|\n",
      "+-------------------+----------------+------------+--------------------+\n",
      "|                135|          339928|         135|      UNITED KINGDOM|\n",
      "|                209|          230840|         209|               JAPAN|\n",
      "|                111|          175445|         111|              FRANCE|\n",
      "|                582|          163778|         582|MEXICO Air Sea, a...|\n",
      "|                245|          159887|         245|          CHINA, PRC|\n",
      "|                112|          146230|         112|             GERMANY|\n",
      "|                689|          128960|         689|              BRAZIL|\n",
      "|                276|          116534|         276|         SOUTH KOREA|\n",
      "|                438|           99731|         438|           AUSTRALIA|\n",
      "|                213|           87988|         213|               INDIA|\n",
      "+-------------------+----------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#From which country (or countries) travelers originate? Top countries of origin.\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM\n",
    "        (SELECT \n",
    "            fi.origin_country_code AS origin_country_code, \n",
    "            COUNT(*) AS country_visitors\n",
    "        FROM fact_immigrations fi \n",
    "        GROUP BY fi.origin_country_code\n",
    "        ORDER BY country_visitors DESC\n",
    "        LIMIT 10\n",
    "        ) AS tcv\n",
    "    JOIN dim_country dc\n",
    "        ON tcv.origin_country_code = dc.country_code\n",
    "    ORDER BY country_visitors DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------+------------+--------------------+\n",
      "|origin_country_code|student_visitors|country_code|        country_name|\n",
      "+-------------------+----------------+------------+--------------------+\n",
      "|                245|            9760|         245|          CHINA, PRC|\n",
      "|                213|            2683|         213|               INDIA|\n",
      "|                276|            2498|         276|         SOUTH KOREA|\n",
      "|                209|            2157|         209|               JAPAN|\n",
      "|                582|            1817|         582|MEXICO Air Sea, a...|\n",
      "|                689|            1677|         689|              BRAZIL|\n",
      "|                261|            1425|         261|        SAUDI ARABIA|\n",
      "|                268|            1060|         268|              TAIWAN|\n",
      "|                696|             890|         696|           VENEZUELA|\n",
      "|                691|             756|         691|            COLOMBIA|\n",
      "+-------------------+----------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top countries from where students are coming?\n",
    "# visa_category_code for student is 3 (SAS file)\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM\n",
    "        (SELECT \n",
    "            fi.origin_country_code AS origin_country_code, \n",
    "            COUNT(*) AS student_visitors\n",
    "        FROM fact_immigrations fi \n",
    "        WHERE visa_category_code = 3\n",
    "        GROUP BY fi.origin_country_code\n",
    "        ORDER BY student_visitors DESC\n",
    "        LIMIT 10\n",
    "        ) AS tcv\n",
    "    JOIN dim_country dc\n",
    "        ON tcv.origin_country_code = dc.country_code\n",
    "    ORDER BY student_visitors DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
